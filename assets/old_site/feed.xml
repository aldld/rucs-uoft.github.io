<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RUCS</title>
    <description>RUCS is a non-archival publication featuring the best of undergraduate computer science research at the University of Toronto.</description>
    <link>rucs.ca/</link>
    <atom:link href="rucs.ca/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 10 Aug 2016 20:08:47 -0400</pubDate>
    <lastBuildDate>Wed, 10 Aug 2016 20:08:47 -0400</lastBuildDate>
    <generator>Jekyll v3.1.3</generator>
    
      <item>
        <title>Visually Simulating Goal Models over Time</title>
        <description>&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;
&lt;script type=&quot;text/javascript&quot;
  src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;
&lt;/script&gt;


&lt;h1 id=&quot;introduction&quot;&gt;I. Introduction&lt;/h1&gt;
&lt;p&gt;In requirements engineering, a goal model is used for specifying the requirements of a software system in terms of the intentions of various stakeholders. A goal model consists of top-level goals that are decomposed into subgoals and tasks, with the relationships between these elements indicated with links connecting them. A goal model can be implemented using the i* modelling language &lt;span class=&quot;citation&quot;&gt;[1]&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;There are simple analysis techniques that use these i* models to help stakeholders make decisions which best lead to the satisfaction of their goals. One such technique is &lt;em&gt;forward analysis&lt;/em&gt;, where discrete values such as &lt;em&gt;satisfied&lt;/em&gt;, &lt;em&gt;denied&lt;/em&gt; and &lt;em&gt;unknown&lt;/em&gt; are assigned to the leaf-level goals and tasks in the model &lt;span class=&quot;citation&quot;&gt;[2]&lt;/span&gt;. These values can then be propagated up the tree to determine the satisfaction of the higher-level goals.&lt;/p&gt;
&lt;p&gt;While useful, this technique is limited when attempting to answer questions about how best to satisfy goals in the model, as the satisfaction values are constant after being assigned. This assumption that the values will not change overtime is not an accurate representation of the real world, where both the requirements of a system and the environment it exists in can frequently change. Reasoning using these models might lead to design decisions which are not the best in the long run.&lt;/p&gt;
&lt;p&gt;For example, if a modeller wished to know if their design would still be functioning over a multi-year period of time, standard analysis techniques might only show whether the design is satisfactory under present conditions. This could lead them to a design decision that would not satisfy all of the requirements over a longer period of time.&lt;/p&gt;
&lt;p&gt;A method has been proposed to account for this dynamic behaviour by supplementing the i* language with temporal functions in the model &lt;span class=&quot;citation&quot;&gt;[3]&lt;/span&gt;. Instead of using the static values of satisfied, denied, and unknown, we represent the satisfaction value of goals and tasks as discrete functions over time. These functions take discrete time points as inputs and output the satisfaction value of the goal at that time. This extends the analysis technique detailed above with a simulation. As the simulation runs through the time points, the model is updated at each point to reflect all of the satisfaction values for that time.&lt;/p&gt;
&lt;p&gt;In order to test the viability of this method, a tool is needed for users to visually create models, and run the simulation on them.&lt;/p&gt;

&lt;h1 id=&quot;approach&quot;&gt;II. Approach&lt;/h1&gt;
&lt;p&gt;An examination of five existing i* goal modelling tools revealed that they were insufficient for this technique. As they were designed with a different intent, they are not easily extensible. Additionally, the tools took time to learn and required installing supporting software, so users would need to be motivated to use them.&lt;/p&gt;
&lt;p&gt;To mitigate this, we decided to create a web-based tool. This tool would run in a common browser and only require Javascript. Specifically, we used the JointJS diagramming library &lt;span class=&quot;citation&quot;&gt;[4]&lt;/span&gt; as the platform to build the tool on top of. The tool, which runs on an Apache web server, runs Javascript on the client side to reduce the computation and eliminate the storage necessary on the server side. In order to run the simulation, the data is sent to our server to be processed. This is a one-time computation and does not require us to store any long-term data about the models.&lt;/p&gt;
&lt;p&gt;The focus in the design was for the tool to be intuitive and usable with minimal instruction. The tool uses a drag-and-drop interface, so users can simply drag elements such as goals and tasks from the stencil onto the canvas to create their models. Features added to increase the usability of the tool include copy and paste, undo and the ability for users to save and load models. The simulation can be run at any point during the modelling, and allows the user to scroll through the timeline and view the model at any particular point in time.&lt;/p&gt;

&lt;h1 id=&quot;analysis&quot;&gt;III. Analysis&lt;/h1&gt;

&lt;figure&gt;
    &lt;img src=&quot;/assets/JF1.png&quot; alt=&quot;Example of a simple model created in the tool.&quot; /&gt;
    &lt;figcaption&gt;&lt;b&gt;Figure 1:&lt;/b&gt; Example of a simple model created in the tool.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
    &lt;img src=&quot;/assets/JF2.png&quot; alt=&quot;The result of the simulation run on the model from Figure 1.&quot; /&gt;
    &lt;figcaption&gt;&lt;b&gt;Figure 2:&lt;/b&gt; The result of the simulation run on the model from Figure 1.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Figure 1 shows a simple model created in the tool. In the original model, the goal “Have Sandwich” is unsatisfied as the denied values propagate up the model. However, the task “Buy Meat” is associated with a &lt;em&gt;Set-Stay-Set Positive&lt;/em&gt; function, which means its value will change over time to &lt;em&gt;satisfied&lt;/em&gt; and remain that way. One frame of the simulation is shown in Figure 2. The value of the “Buy Meat” goal has become satisfied, and so the satisfaction has propagated up the tree and resulted in our main goal of “Have Sandwich” being satisfied. Notice how the satisfied value on the “Buy Meat” task has propagated to the “Have Sandwich” goal.&lt;/p&gt;
&lt;p&gt;We have used the tool to build medium-sized goal models containing around 20 goals, tasks and subgoals. We have conducted preliminary interviews with five goal modelling experts and have used their feedback to refine the interface and improve the functionality of the tool.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;IV. Conclusion&lt;/h1&gt;
&lt;p&gt;In this project, we built an intuitive tool to add dynamic, time-based considerations to i* goal modelling. The purpose was to allow users to account for changes over time in order to make more informed decisions when reasoning with their models. Work is ongoing to further extend i* and develop further tools in order to give modellers more information to better reason with their goal models and ultimately lead to better design decisions. We are currently planning a user study to evaluate the effectiveness and intuitiveness of the tool with a broader audience.&lt;/p&gt;

&lt;h4&gt;References&lt;/h4&gt;
&lt;div class=&quot;references&quot;&gt;
    &lt;div id=&quot;ref-Yu:1997&quot;&gt;
        &lt;p&gt;[1] E. Yu, “Towards Modeling and Reasoning Support for Early-Phase Requirements Engineering,” in &lt;em&gt;Proc. of rE’97&lt;/em&gt;, 1997, pp. 226–235.&lt;/p&gt;
    &lt;/div&gt;
    &lt;div id=&quot;ref-Horkoff:2010F&quot;&gt;
        &lt;p&gt;[2] J. Horkoff and E. Yu, “Interactive Analysis of Agent-Goal Models in Enterprise Modeling,” &lt;em&gt;Int. J. of Inform. Syst.&lt;/em&gt;, vol. 1, no. 4, pp. 1–23, 2010.&lt;/p&gt;
    &lt;/div&gt;
    &lt;div id=&quot;ref-Grubb:2015&quot;&gt;
        &lt;p&gt;[3] A. M. Grubb, “Adding temporal intention dynamics to goal modeling: A position paper,” in &lt;em&gt;MiSE’15&lt;/em&gt;, 2015.&lt;/p&gt;
    &lt;/div&gt;
    &lt;div id=&quot;ref-jointJS:2015&quot;&gt;
        &lt;p&gt;[4] clientIO, “JointjS.” 2015.&lt;/p&gt;
    &lt;/div&gt;
&lt;/div&gt;
</description>
        <pubDate>Mon, 19 Oct 2015 00:00:00 -0400</pubDate>
        <link>rucs.ca/software-engineering/visually-simulating-goal-models-over-time</link>
        <guid isPermaLink="true">rucs.ca/software-engineering/visually-simulating-goal-models-over-time</guid>
        
        
        <category>Software Engineering</category>
        
      </item>
    
      <item>
        <title>Using Android Wear to Detect Early Onset of Acute COPD Exacerbations</title>
        <description>&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;
&lt;script type=&quot;text/javascript&quot;
  src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;
&lt;/script&gt;


&lt;h1 id=&quot;introduction&quot;&gt;I. Introduction&lt;/h1&gt;
&lt;p&gt;Chronic obstructive pulmonary disease (COPD) is a chronic inflammatory lung disease that causes obstructed airflow from the lungs, primarily caused by smoking tobacco. The disease affects over 329 million people globally &lt;span class=&quot;citation&quot;&gt;[2]&lt;/span&gt;. In 2013, COPD was responsible for 6% of total deaths worldwide, which amounted to over three million &lt;span class=&quot;citation&quot;&gt;[3]&lt;/span&gt;. The total economic loss due to COPD was estimated to be 2.1 trillion USD in 2010. This number is expected to rise to 4.8 trillion USD in 2030 &lt;span class=&quot;citation&quot;&gt;[1]&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;An acute COPD exacerbation is characterized by shortness of breath, coughing and sputum. An exacerbation episode can last several days and is the primary cause for hospitalization in COPD patients. During these episodes, the patient is often unable to walk even a few feet without repeated rests.&lt;/p&gt;
&lt;p&gt;Acute exacerbations take time to manifest and often exhibit predictable symptoms &lt;span class=&quot;citation&quot;&gt;[4]&lt;/span&gt;. Our aim is to use modern mobile technology to assist in detecting the onset of exacerbation early enough to prevent the patient’s health from deteriorating further and avoid hospitalization. This helps achieve a better lifestyle for the patient and dramatically decreases costs for the healthcare system.&lt;/p&gt;

&lt;h1 id=&quot;approach&quot;&gt;II. Approach&lt;/h1&gt;
&lt;p&gt;Our approach involves using an Android wearable application coupled with machine learning to accurately predict an oncoming exacerbation episode. The patient is provided with an Android watch and phone. The watch continually records audio, accelerometer data and heart rate. Patients are also surveyed daily, using an application on the phone, for any change in symptoms. These changes could include: increase in sputum, changes in sputum color, breathless- ness, an onset of a fever, etc.&lt;/p&gt;
&lt;p&gt;In the interest of privacy, the application on the phone provides an easy-to-use interface for patients to delete any recordings they may not want us to use. The data is initially stored locally on the phone where we analyze the audio and other data to extract the essential portions required for our study. This could include portions of audio with coughs, other symptom-related characteristics, and time intervals with elevated heart rate. The extracted data is encrypted and sent to our servers where it is further analyzed by the classifier to detect any signs of exacerbation.&lt;/p&gt;
&lt;p&gt;Wearable devices are often battery constrained. A typical modern smartwatch would last 12-16 hours on a full charge. In earlier tests we found that recording data continuously from the microphone and sensors would deplete the battery in as little as 3-4 hours. Since users typically only charge the device once a day at night, this approach would lead us to losing incredibly important data. To counter this, we set our application to record for two minutes and then put it to sleep for four minutes. This allowed us to reach our target battery goal of 10-12 hours and provided us with a regular, albeit non-continuous, stream of data throughout the day.&lt;/p&gt;
&lt;p&gt;The first phase of our deployment targets a small set of patients in a hospital. This enables us to have easy access to them to supervise the tests and continually improve the application with their feedback. The data we collect here would be used to initially train the classifier. Afterwards, we plan to expand the application to a much wider audience for use in their homes.&lt;/p&gt;
&lt;p&gt;Our initial batch of patients will test the application in Fall 2015.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;III. Conclusion&lt;/h1&gt;
&lt;p&gt;Though we are still in the early stages of the project, we are extremely optimistic about future outcomes. A similar experiment which used just a survey as its primary method showed a significant decrease in hospitalizations during the trial. Our use of smart but ubiquitous devices, a watch and smartphone, allows us to remain noninvasive, all the while providing us the ability to collect most of the data automatically and without the need for continuous input. This should increase both the input’s accuracy and regularity.&lt;/p&gt;

&lt;h4&gt;References&lt;/h4&gt;
&lt;div class=&quot;references&quot;&gt;
    &lt;p&gt;[1] Lomborg, Bjørn &lt;span&gt;&lt;em&gt;Global problems, smart solutions: costs and benefits.&lt;/em&gt;&lt;/span&gt; 2013.&lt;/p&gt;
    &lt;p&gt;[2] Salvi, Sundeep &lt;span&gt;&lt;em&gt;The silent epidemic of COPD in Africa.&lt;/em&gt;&lt;/span&gt; 2015&lt;/p&gt;
    &lt;p&gt;[3] World Health Organization &lt;span&gt;&lt;em&gt;Chronic obstructive pulmonary disease (COPD).&lt;/em&gt;&lt;/span&gt; 2015&lt;/p&gt;
    &lt;p&gt;[4] Wise, A. Robert &lt;span&gt;&lt;em&gt;Chronic Obstructive Pulmonary Disease (COPD).&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;</description>
        <pubDate>Mon, 19 Oct 2015 00:00:00 -0400</pubDate>
        <link>rucs.ca/computational-biology/using-android-wear-to-detect-early-onset-of-acute-COPD-exacerbations</link>
        <guid isPermaLink="true">rucs.ca/computational-biology/using-android-wear-to-detect-early-onset-of-acute-COPD-exacerbations</guid>
        
        
        <category>Computational Biology</category>
        
      </item>
    
      <item>
        <title>SQL Visualizer</title>
        <description>&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;
&lt;script type=&quot;text/javascript&quot;
  src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;
&lt;/script&gt;

&lt;h1 id=&quot;introduction&quot; class=&quot;unnumbered&quot;&gt;I. Introduction&lt;/h1&gt;
&lt;p&gt;In many computer science programs, students encounter courses on databases, many of which use Sequential Query Language (SQL). However, most database management systems (DBMSes) which execute SQL do not provide the ability to step through parts of a query, making it difficult to pinpoint sources of error. In order to help beginners master the core of SQL, we built a visualizer which allows users to trace through the execution of one or more queries, similar to a debugger for other languages. The visual representation also makes it easier to understand more difficult aspects of the language, such as nested queries.&lt;/p&gt;

&lt;h1 id=&quot;visualization&quot; class=&quot;unnumbered&quot;&gt;II. Visualization&lt;/h1&gt;
&lt;p&gt;Queries are decomposed into steps which are ordered according to the order of execution suggested by the PostgreSQL 9.3 documentation&lt;a href=&quot;#fn1&quot; class=&quot;footnoteRef&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. These steps are presented as a table of contents that the user can navigate (see ”Navigation Bar” in Figure 1).&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/assets/KGSHMP1.png&quot; alt=&quot;The visualizer and the components of its interface.&quot; /&gt;
    &lt;figcaption&gt;&lt;b&gt;Figure 1:&lt;/b&gt; The visualizer and the components of its interface.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;For each step, students can examine the table that the step produces (the output), the tables used to generate it (the input), and the attributes available (the namespace). To view a specific step, students can click on it in the navigation bar. Alternatively, the navigational arrows can be used to traverse steps chronologically.&lt;/p&gt;
&lt;p&gt;The visualizer makes explicit the namespace of tables, views and attributes that can be referenced at any given time in the execution of a query. Consider the schema represented in the Tables and Views tab in Figure 1:
&lt;blockquote&gt;
    &lt;code&gt;
        
            Offering (oid, dept, cNum, instructor)&lt;br/&gt;
            Took (sid, oid, grade)&lt;br/&gt;
            Student (sid, firstName, email, cgpa)&lt;br/&gt;
            Course (dept, cNum, name)&lt;br/&gt; 
    &lt;/code&gt;
&lt;/blockquote&gt;
and the query:&lt;/p&gt;
&lt;blockquote&gt;
    &lt;code&gt;SELECT sid FROM Student;&lt;/code&gt;
&lt;/blockquote&gt;
&lt;p&gt;The first step of the query is “FROM Student” which introduces the table ”Student” to the namespace, which becomes:&lt;/p&gt;
&lt;blockquote&gt;
    &lt;code&gt;Student: sid, firstName, email, cgpa&lt;/code&gt;
&lt;/blockquote&gt;
    &lt;p&gt;Though other tables and their attributes exist, none of them can be referred to. In our example, “SELECT sid” would then pare down the namespace into:&lt;/p&gt;
&lt;blockquote&gt;
    &lt;code&gt;Student: sid&lt;/code&gt;
&lt;/blockquote&gt;
&lt;p&gt;However, including ”dept” in the SELECT clause would raise a syntax error. Even though “dept” exists in the schema, the visualizer clearly shows that it is not in the namespace and therefore cannot be referenced.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/assets/KGSHMP2.png&quot; alt=&quot;The query SELECT * FROM Student WHERE EXISTS(SELECT oid FROM Offering WHERE oid &amp;gt; sid with the subquery open. The row that opened the subquery has an sid of 1.&quot; /&gt;
    &lt;figcaption&gt;&lt;b&gt;Figure 2:&lt;/b&gt; The query “SELECT * FROM Student WHERE EXISTS(SELECT oid FROM Offering WHERE oid &amp;gt; sid” with the subquery open. The row that opened the subquery has an sid of 1.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In queries with a WHERE clause, the reasons why certain rows of a table are kept may not be obvious. As a WHERE clause filters out rows that do not satisfy its conditions, displaying these conditions allows students to better understand why any given row is kept or discarded. The visualizer colours rows green if they are kept and red if they are not, as seen in Figure 1. By clicking on a row, the user brings up a tool tip which lists the conditions a row passed or failed, making the reasons it was kept or discarded explicit.&lt;/p&gt;
&lt;p&gt;Nested queries can be difficult for students to understand. One challenge is discerning which attributes are referenceable in relation to the scope of a subquery. The user can choose to examine a subquery in a modal window that shows steps in a manner identical to the parent query, as seen in Figure 2. Although the subquery in Figure 2 only mentions the table “Offering” in the FROM clause, “Student” also appears in the namespace, as the parent query introduced it. The visualizer makes it clear that a subquery inherits its parent’s namespace. After returning to the parent query, the subquery’s namespace disappears, a behaviour analogous to function calls.&lt;/p&gt;

&lt;h1 id=&quot;software-architecture&quot; class=&quot;unnumbered&quot;&gt;III. Software Architecture&lt;/h1&gt;
&lt;p&gt;Our visualizer has three components: a parser, interpreter, and renderer. The parser, used to convert a query into an abstract syntax tree, was built by extending PyParsing’s SQL demo grammar according to the syntax supported by PostgreSQL 9.3. The abstract syntax tree is interpreted by our software to produce the steps involved in executing the query (according to the order of execution, as mentioned above), and the table resulting from the execution of each step. This information is then represented in JSON, from which a visualization is generated.&lt;/p&gt;
&lt;p&gt;The query is processed all at once, creating every table before any visualization happens, as opposed to generating tables only when a user wants to see a step. At the cost of a slightly longer loading time, users gain the ability to explore or re-explore steps in any order without delay.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/assets/KGSHMP3.png&quot; alt=&quot;Software Architecture&quot; /&gt;
    &lt;figcaption&gt;&lt;b&gt;Figure 3:&lt;/b&gt; Software Architecture&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;conclusion&quot; class=&quot;unnumbered&quot;&gt;IV. Conclusion&lt;/h1&gt;
&lt;p&gt;Our visualizer breaks down queries, providing a clear image of how queries are processed to produce the final result. Although allowed in principle, advanced features of SQL, such as WITH clauses and multiple levels of nested subqueries, are not yet supported. The visualizer is intended for beginners, rather than thorough debugging, so its focus is on covering core features. Future endeavors include extending the visualization to handle relational algebra, as well as using the visualizer in the introductory database course at the University of Toronto.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
&lt;hr /&gt;
&lt;ol&gt;
    &lt;li id=&quot;fn1&quot;&gt;&lt;p&gt;http://www.postgresql.org/docs/9.3/interactive/sql-select.html&lt;a href=&quot;#fnref1&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Mon, 19 Oct 2015 00:00:00 -0400</pubDate>
        <link>rucs.ca/software-projects/SQL-visualizer</link>
        <guid isPermaLink="true">rucs.ca/software-projects/SQL-visualizer</guid>
        
        
        <category>Software Projects</category>
        
      </item>
    
      <item>
        <title>Predicting Exam Results in an Introductory Level Computer Science Course</title>
        <description>&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;
&lt;script type=&quot;text/javascript&quot;
  src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;
&lt;/script&gt;

&lt;h1 id=&quot;introduction&quot;&gt;I. Introduction&lt;/h1&gt;
&lt;p&gt;In recent years, systems featuring automatically marked online programming exercises have become increasingly common. These automated systems generate large amounts of data that can be mined for insight on student programming behavior and to make predictions on student performance.&lt;/p&gt;
&lt;p&gt;Predictors of student performance based on programming exercises have become increasingly sophisticated. Jadud’s early work focused on the use of syntax errors and found correlations between term marks and programming errors. Watson et al. extended these ideas by incorporating the time spent solving problems, increasing their predictive power. Most recently, Carter et al. reported on a heuristic that accounts for some semantic correctness. They claim that a combination of programming practices and errors at the IDE level can be used to detect struggling students in introductory level computer science courses earlier on. All of these measures were applied to relatively large programming assignments.&lt;/p&gt;
&lt;p&gt;In contrast, earlier this year, Spacco et al. investigated participation and performance data from short, web-based programming exercises and reported correlations between performance on these exercises and overall performance in the course. Similarly, we investigated short, online exercises, but instead of focusing on participation, we considered the number of attempts required to complete problems successfully.&lt;/p&gt;

&lt;h1 id=&quot;approach&quot;&gt;II. Approach&lt;/h1&gt;
&lt;p&gt;Our data set includes exercises completed by 368 students in a first year programming course at a research-intensive university. Students who did not complete the course (including the exam) or who scored below 30% (indicating a lack of engagement in the course) were removed. The course is taught in Python and features content ranging from variable assignment to loops and module-level design. It attracts students from a wide range of ability levels, including students with no prior programming experience. While the course is a pre-requisite for many higher-level computer science courses, it also attracts students completing electives. The course features a midterm and a final exam. In addition to these assessments, students also complete 42 coding and multiple-choice questions spread across weekly problem sets and submit three longer assignments that are not completed online.&lt;/p&gt;
&lt;p&gt;We analyzed the multiple choice questions by calculating the number of questions a student completed under the average amount of attempts required. Spacco et al. reported that in the majority of computer science courses they investigated, there was a statistically significant (p-value &amp;lt; 0.05) relationship between a student’s final exam mark and the number of questions attempted as well as the number of questions completed correctly. However, in our data set, students were allowed to retry questions until they got a correct answer, so, unlike Spacco et al.’s study, most students were able to submit a correct answer to every attempted question. As a result, we used the average number of attempts to identify students with a stronger understanding of the concepts being evaluated.&lt;/p&gt;
&lt;p&gt;We analyzed the coding exercises by using linear regression to identify the programming exercises with the strongest relationship with exam performance. Our analysis identified six exercises (of twenty) with a strong correlation to final exam marks, and a review of these six exercises confirms that they are more directly comparable to questions on the midterm and exam in terms of difficulty and content coverage.&lt;/p&gt;
&lt;p&gt;We combine these metrics to predict student performance. Our metric combines the number of multiple-choice questions completed within the average number of attempts and correctness on the six coding questions to generate a prediction score.&lt;/p&gt;

&lt;h1 id=&quot;results-and-analysis&quot;&gt;III. Results and Analysis&lt;/h1&gt;
&lt;p&gt;Overall, a weak correlation – comparable to those found by Watson et al. – exists between a student’s prediction score and final exam mark. The results are significant (p-value &amp;lt; 0.001), with an adjusted R-squared value of 0.2956. The F-statistic is 21.16 on 7 and 353 degrees of freedom.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/assets/SZ1.png&quot; alt=&quot;&quot; /&gt;
    &lt;figcaption&gt;&lt;b&gt;Figure 1:&lt;/b&gt; Final exam marks vs prediction score.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The questions evaluated are similar in structure and content to those on the final exam. Furthermore, they are assessments, as they were evaluated after content was taught and practiced. This suggests that performance on individual, unsupervised exercises in a low-stress environment can be predictive of performance on similar problems in a summative assessment. Successfully completing these low stakes questions without guessing or needing excessive numbers of attempts indicates that a student will be able to apply similar concepts in an exam environment.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;IV. Conclusion&lt;/h1&gt;
&lt;p&gt;There is a weak but statistically significant correlation between performance on online exercises and final exam performance, showing that there is promise in using automated online assignments in predicting exam results. Spacco et al. [3] show that statistically significant relationships existed between participation in online assignments and exam performance, and we have identified that much stronger correlations – on the order of those discovered in data from larger assignments – can be found by focusing on relative performance and selecting coding exercises with a similar difficulty level to the summative assessments. Instructors may be able to assign such problems in a course and then apply similar analysis techniques while the course is ongoing to identify students at risk of failing the exam.&lt;/p&gt;

&lt;h4 id=&quot;references&quot;&gt;References&lt;/h4&gt;
&lt;div class=&quot;references&quot;&gt;
    &lt;p&gt;[1] M. C. Jadud, &quot;A First Look at Novice Compilation Behaviour Using BlueJ,&quot; Computer Science Education, vol. 15, 2005.&lt;/p&gt;
    &lt;p&gt;[2] C. Watson, F. W. Li and J. L. Godwin, &quot;No Tests Required: Comparing Traditional and Dynamic Predictors of Programming Success,&quot; Proceedings of the 45th ACM technical symposium on Computer science education, 2014.&lt;/p&gt;
    &lt;p&gt;[3] A. S. Carter, C. D. Hundhausen and O. Adesope, &quot;The Normalized Programming State Model: Predicting Student Performance in Computing Courses Based on Programming Behavior,&quot; Proceedings of the eleventh annual International Conference on International Computing Education Research, 2015.&lt;/p&gt;
    &lt;p&gt;[4] J. Spacco, P. Denny, B. Richards, D. Babcock, D. Hovemeyer and J. Moscola, &quot;Analyzing Student Work Patterns Using Programming Exercise Data,&quot; Proceedings of the 46th ACM Technical Symposium on Computer Science Education, 2015.&lt;/p&gt;
&lt;/div&gt;



</description>
        <pubDate>Mon, 19 Oct 2015 00:00:00 -0400</pubDate>
        <link>rucs.ca/software-projects/predicting-exam-results</link>
        <guid isPermaLink="true">rucs.ca/software-projects/predicting-exam-results</guid>
        
        
        <category>Software Projects</category>
        
      </item>
    
      <item>
        <title>N-way Model Merging Game</title>
        <description>&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;
&lt;script type=&quot;text/javascript&quot;
  src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;
&lt;/script&gt;

&lt;h1 class=&quot;introduction&quot;&gt;I. Introduction&lt;/h1&gt;

&lt;figure&gt;
    &lt;img src=&quot;/assets/CC1.png&quot; alt=&quot;&quot; /&gt;
    &lt;figcaption&gt;&lt;b&gt;Figure 1:&lt;/b&gt; Models &lt;span class=&quot;math inline&quot;&gt;\(M_{1},M_{2},M_{3}\)&lt;/span&gt; are combined into a single model &lt;span class=&quot;math inline&quot;&gt;\(M_{1} + M_{2} + M_{3}\)&lt;/span&gt; through the process of NwM.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;N-way model merging (NwM) [1] is the process of merging sets of software models into one by grouping classes that have similar attributes. Class names, names of class attributes, and relationships with other classes are considered to be model attributes. An optimal solution to NwM produces a result in which the chosen groupings have the highest degree of similarity. An example of NwM is depicted in Figure 1, in which models &lt;span class=&quot;math inline&quot;&gt;\(M_{1},M_{2},M_{3}\)&lt;/span&gt; are combined into a single model&lt;span class=&quot;math inline&quot;&gt;\(\ M_{1} + M_{2} + M_{3}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;NwM is an NP-hard problem; that is, it’s in the class of problems that have no known polynomial run time algorithm, making it impossible to automate. This is unfortunate, as NwM has a variety of useful applications in software engineering, such as combining related products in a product line and consolidating the views of multiple stakeholders. As a result, previous studies run, such as the one conducted by Julia Rubin and Marsha Chechik [1], attempted to devise heuristic algorithms that approximate NwM. Although these algorithms run in polynomial time, they are not guaranteed to compute the optimal solution. This paper explores an alternative approach that may perform better than these heuristic algorithms.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/assets/CC2.png&quot; alt=&quot;&quot; /&gt;
    &lt;figcaption&gt;&lt;b&gt;Figure 2:&lt;/b&gt; In the “serious game”, players will group sets of objects encoded as software models. This diagram shows the process of merging two models of fruits and vegetables.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 class=&quot;approach&quot;&gt;II. Approach&lt;/h1&gt;
&lt;p&gt;Our approach utilizes the human ability of detecting visual similarities and differences and involves creating a “serious game” that will require players to perform NwM. Instead of making users merge UML models such as the ones in Figure 1, players will be merging sets of objects, such as the fruits and vegetables shown in Figure 2. These objects will be encoded as software models.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/assets/CC3.png&quot; alt=&quot;&quot; /&gt;
    &lt;figcaption&gt;&lt;b&gt;Figure 3:&lt;/b&gt; Objects created.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;We experimented with a variety of objects but narrowed it down to bacteria sets (1), aliens (2), and alien families (3). These are shown in Figure 3. For example, one alien would represent a single class, and each feature of the alien would represent an attribute. A set of bacteria would represent a single class, and each individual bacterium would represent an attribute.&lt;/p&gt;
&lt;p&gt;We wanted to determine whether humans would perform differently if objects were presented as a set of items, as in the case of bacteria, or if they were presented as a single entity with superimposed items, as in the case of aliens. The software models in the game have a total of 162 attributes. Alien families were created because it was hard to fit 162 attributes on a single alien and thus the alien attributes had to be made small in order to fit all of them. This made it difficult for players to notice all attributes on the aliens. With alien families, the 162 attributes could be spread out over 4 aliens, making them bigger and easier to notice.&lt;/p&gt;

&lt;h1 class=&quot;analysis&quot;&gt;III. Analysis&lt;/h1&gt;
&lt;p&gt;We conducted a University of Toronto ethics-approved user study to determine which of the object types would be most useful for the game. The study aimed to determine whether participants would correctly guess the most similar pair when presented with multiple game objects on the screen. In total, 164 participants successfully completed the study (each taking about an hour), and the results suggest that players perform the best on aliens, with bacteria sets following closely behind. We believe that participants may not have done as well as they could have on bacteria sets due to the “tiring effect”, as these objects were presented last in the questionnaire. Therefore, we are considering both aliens and bacteria as our final game objects.&lt;/p&gt;

&lt;h1&gt;IV. Future Work&lt;/h1&gt;
&lt;p&gt;We are currently developing an actual game with the objects we created. A challenge that we face is getting the game to scale for large software models, as it will become increasingly difficult for players to keep track of all the objects on the screen. This necessitates a design that will prevent players from feeling overwhelmed with information when playing the game. The design will also need to account for the fact that it becomes harder to fit all objects on the screen as models increase in size. Since NvM is not an inherently interesting task, the gameplay must also be made enticing for players.&lt;/p&gt;

&lt;h4&gt;References&lt;/h4&gt;
&lt;div class=&quot;references&quot;&gt;
    &lt;p&gt;[1] J. Rubin and M. Chechik. N-way Model Merging. &lt;em&gt;In Proc. of ESEC/FSE’13&lt;/em&gt;, pages 301-311, 2013.&lt;/p&gt;
&lt;/div&gt;
</description>
        <pubDate>Mon, 19 Oct 2015 00:00:00 -0400</pubDate>
        <link>rucs.ca/software-engineering/n-way-model-merging-game</link>
        <guid isPermaLink="true">rucs.ca/software-engineering/n-way-model-merging-game</guid>
        
        
        <category>Software Engineering</category>
        
      </item>
    
      <item>
        <title>!Mediengruppe Bitnik:&lt;br/&gt;  The Internet as an Artistic Medium</title>
        <description>&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;

&lt;br/&gt;&lt;br/&gt;
&lt;p&gt;&lt;em&gt;!Mediengruppe Bitnik (read: the not Mediengruppe Bitnik) is an art collective based in Zurich and London led by Carmen Weisskopf and Domagoj Smoljo. With the help of London filmmaker and researcher Adnan Hadzi and reporter Daniel Ryser, this group of contemporary artists plays with the Internet as their primary medium. Among their numerous works is what they described as a SYSTEM_TEST, a “32-hour live mail art piece” entitled Delivery for Mr. Assange. Six months after Julian Assange was granted political asylum at the Ecuadorian embassy in London, the artists decided to infiltrate the warzone with a parcel. It contained nothing but a camera that took pictures through a small hole and uploaded the photos online as its journey progressed. The final frames were of Assange himself, holding up messages he wrote on notecards for the world to see.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Their most famous project, however, is the Random Darknet Shopper, a software bot they unleashed on a deep web marketplace near the end of last year. For three months, it randomly chose one item to purchase within its weekly budget of one hundred dollars in Bitcoins; the artists were never sure what to expect. Their final collection included jeans, a scanned Hungarian passport, a baseball cap with a hidden camera, and ten pills of ecstasy, which were seized and destroyed by the police.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;b&gt;How did !Mediengruppe Bitnik begin? What’s the story behind your name?&lt;/b&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We met at [an] art university and started experimenting with the possibilities that the Internet brings to the arts. The Internet was just starting to become big; it was fascinating to have a[n] easily accessible, affordable medium within our reach, which allowed us to publish, to connect and to share.&lt;/p&gt;
&lt;p&gt;Bitnik was actually the name of our first server. We secretly introduced this machine into the art school&#39;s computer network. Suddenly the machine – bitnik – was visible 24/7 worldwide, something that was still kind of rare in those days. That was exhilarating!&lt;/p&gt;
&lt;p&gt;The name Bitnik is derived from Bit, the smallest digital unit and -nik as in Sputnik&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br/&gt;&lt;b&gt;Why did you choose to work on or with the Internet in particular?&lt;/b&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For us as young artists, the Internet made it incredibly easy to become part of a network, to publish, to have a voice. It was easy to communicate, to share with others, to get involved. From very early on, we were fascinated with the online subculture, permanently evolving and which over time also started massively influencing offline culture.&lt;/p&gt;
&lt;p&gt;From early on we focussed our interests in the spaces formed by the intersection of offline and online. We were fascinated by the immaterial and the questions it raises within the arts. At the same time we also aimed at materializing our concepts and at introducing some of the online humour into the arts.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br/&gt;&lt;b&gt;Why does the URL to your website begin with 22 Ws?&lt;/b&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This is a small irritation, a breach of conventions, a little craziness introduced to keep the Internet from becoming boring.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br/&gt;&lt;b&gt;What was the inspiration for the Random Darknet Shopper?&lt;/b&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The Random Darknet Shopper was part of an exhibition we co-organized called «The Darknet – From Memes to Onionland. An Exploration».&lt;/p&gt;
&lt;p&gt;The idea behind the exhibition was to explore the Darknet from an artistic viewpoint, also hoping to critically evaluate mass surveillance and to study alternative structures and forms of communicating outside mass surveillance. How is identity formed within these networks? How is communication and exchange possible in anonymous networks? What forms of trust building arise? How do you trust each other if you don&#39;t know to whom you are talking to [sic]? How can we as artists examine these questions in a meaningful way?&lt;/p&gt;
&lt;p&gt;For the exhibition we realized the Random Darknet Shopper, a shopping bot which randomly bought things in the deep web and had it delivered directly to the exhibition space.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br/&gt;&lt;b&gt;What was your reaction upon discovering that the bot had purchased ecstasy?&lt;/b&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Together with a lawyer specialising in Art Law, we had already evaluated the work beforehand. We knew that there could be some purchases made by the bot that would be questionable from a legal point of view. But our lawyer encouraged us to go through with the work, because he believes that we are raising important questions and that the possible breach of law is a necessary means for raising these questions.&lt;/p&gt;
&lt;p&gt;We handled the ecstasy like all the other items: We unpacked it and hung it into its display box. To receive a new item from the Darknet every week was really exciting, but also nerve-wracking, because we were never quite sure what we were handling.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br/&gt;&lt;b&gt;How did you implement the bot? That is, what approach did you take to coding it?&lt;/b&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Technically the thing is not so challenging - imagine a simple crawler written specifically for the Agora marketplace, automating Firefox browser interaction with the site.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br/&gt;&lt;b&gt;Why did you claim responsibility for an illegal purchase which was made by a bot that was behaving randomly?&lt;/b&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;After the work was seized by local police, it was mandatory to claim ownership of the work and thus responsibility in order to be able to act on the seizure. Being an art piece, authorship of the work and the bot was clear from the start, so it would have made no sense to hide behind the bot&#39;s randomness. Although autonomy of digital systems does raise interesting questions around responsibility and accountability.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br/&gt;&lt;b&gt;At what point do you think the creator of an artificial intelligence stops being accountable for the actions of the creation?&lt;/b&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This will be one of the interesting questions that we will have to deal with in the near future. With an artistic work like the Random Darknet Shopper, the question of responsibility may still be fairly easy to answer. But how about trading bots? Do you make every company, every programmer involved responsible? Are they also responsible for future developments they may not have foreseen? Or self-driving cars: what if the software fails? Who is responsible for unforeseeable security flaws stemming from [the] fact that these systems are getting more and more complex?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br/&gt;&lt;b&gt;What challenges exist when the Internet is a part of the medium? How did you overcome them for this project?&lt;/b&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What makes the Internet such an interesting media for us as artists is also that it is not static – it is constantly evolving. The Internet is shaped by the societies using it, regulating it. This can be challenging. In many of our works, we consciously work with an element of randomness, of loss of control. This is interesting but also challenging.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br/&gt;&lt;b&gt;Many of your projects, including this one, have a live aspect to them where an audience can watch the whole thing come together. Why is that?&lt;/b&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The live media and performative elements in our works let us create situations where interesting processes are set in motion. In this sense you could say that all our work is time-based. Time is always an important factor. It is important to us that these processes be open-ended and non-deterministic. That way, the workings of the systems used become visible. Also, it is the liveness of the media, the development of the performance in real-time, which unfolds an inclusive power. Look at the work &lt;em&gt;Delivery for Mr. Assange&lt;/em&gt; for example: The images from the parcel were uploaded directly to a public website in real time. So everyone else following the parcel&#39;s journey actually had the same information of what was happening as we did. This led to people narrating their own stories, taking the piece and expanding on it. This opens up a space for reflection and for action which is very valuable to us. The usually controlled situation becomes out of control; it is not easily foreseeable. For us, this is also a way of countering a prevalent belief in the rhetoric of security and a predominantly deterministic worldview. We call this approach RRRRRRRRRRadical Realtime.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br/&gt;&lt;b&gt;What did you hope people would take away from your exhibition? &lt;/b&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The 12 works by various artists we showed in the exhibition «The Darknet – From Memes to Onionland. An Exploration» all approached the topics involved from different angles. We hope the artistic view on questions of visibility, anonymity, privacy, intimacy and security offered people a more sensual approach to these topics. The overwhelming public interest shows that there is little out there that allows a different – more humorous – approach.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Mon, 19 Oct 2015 00:00:00 -0400</pubDate>
        <link>rucs.ca/profiles-and-interviews/mediengruppe-bitnik</link>
        <guid isPermaLink="true">rucs.ca/profiles-and-interviews/mediengruppe-bitnik</guid>
        
        
        <category>Profiles and Interviews</category>
        
      </item>
    
      <item>
        <title>Marc Andreessen:&lt;br/&gt;  A Futurist and an Optimist</title>
        <description>&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;

&lt;br/&gt;&lt;br/&gt;
&lt;p&gt;Marc Andreessen is a man with his head in the clouds. This isn’t because he’s 6’4, but because he has endless predictions for the future, many of them seemingly farfetched, even frightening. But he is certainly a man worth listening to. There are only six people who have been inducted into the World Wide Web Hall of Fame, and Andreessen is among them. He is a co-author of Mosaic, the first widely used web browser, and co-founder of Netscape, the software company built around Mosaic. Today, he is a co-founder and general partner at Andreessen Horowitz (commonly known as a16z), an eminent venture capital firm in Silicon Valley. Andreessen jokes about his squeaky voice, but when he speaks, everyone in the tech world listens.&lt;/p&gt;
&lt;p&gt;The powerful and influential Andreessen lives a fairly quiet life. He has non-tech interests in classical music, philosophy and history, and spends most nights watching television with his wife Laura Arrillaga-Andreessen, a lecturer in philanthropy at Stanford University who cites her husband’s bald head as one of his best qualities. Andreessen and his wife hardly go out, rarely travel, and once his wife is asleep, Andreessen works through the night. He describes different stages of his life as operating system versions. He is no longer an imitation of his business partner and former mentor, Jim Clark (Marc 1.0), and past his phase as a refined executive (Marc 2.0). As Marc 3.0, “The goal is not to be elegant but to be blunt enough that there’s no confusion,” explains the venture capitalist, who became a father in March of 2015. What has been consistent, from his days as a misunderstood boy in the rural American Midwest, to his current status as an investor and millionaire, is his fascination with technology and its possibilities.&lt;/p&gt;
&lt;p&gt;In an interview with Robert Scoble of Rackspace Studios, Andreessen was asked what gets him out of bed every morning:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“You never know when the next really breakthrough [sic] idea is going to walk through the door… The history of the big breakthrough companies that get built is that they do tend to raise venture capital at some point…they do, by the way, tend to get told “no” an awful lot…they get told “no” by virtually every possible investor, until someone finally says yes, so you literally never know. It’s hard to miss a day at work, because you never know if that’s the day the next Mark Zuckerberg or the next Bill Gates is going to walk through the door. And think about how badly you would kick yourself if you were sleeping that day. For me, that’s motivating.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Andreessen has said that he and his fellow partners at a16z spend a lot of time saying “no”, but they’ve also said “yes” to a lot of successful startups like AirBnB, Pinterest, Buzzfeed and Slack. He uses a baseball analogy to describe the venture capital business: “It’s not a batting average business, it’s a slugging percentage business. It’s not about how often you’re right, it’s about, when you’re right, how right are you?”&lt;/p&gt;
&lt;p&gt;When assessing the three key elements of a startup – team, product and market – Andreessen prioritizes the third as the key factor to a startup’s success. “The market pulls product out of a startup. The market needs to be fulfilled and the market will be fulfilled by the first viable product that comes along,” he explains. Sometimes, the market simply isn’t ready for a product, and Andreessen can relate to this notion personally. In 1999, he co-founded Loudcloud with Jim Clark, with whom he also founded Netscape. Loudcloud was the first company to offer cloud storage, but it wasn’t successful and did not last long; the market wasn’t ready. A good idea was defeated by bad timing. A decade later, cloud computing is ubiquitous.&lt;/p&gt;
&lt;p&gt;For all the praise he receives, Andreessen has plenty of critics and challengers. When debating Peter Thiel, co-founder of Paypal and venture capitalist, Thiel challenged Andreessen’s optimism about innovation, suggesting that we have stagnated in recent decades, and most recent innovations – he uses the game Angry Birds as an example – have been trivial. Where Andreessen is happy to make predictions about the near future, Thiel says that since the 1970’s, we’ve been “on the cusp” on big breakthroughs that haven’t occurred yet, like curing cancer. Where Andreessen’s views Twitter as “instant global public messaging, for free” as a massive communications breakthrough, Thiel is skeptical of whether the social network is an important innovation that improves the economy and living standards. After predicting a positive future for the news business due to the increase of journalism entrepreneurship and growing number of people with smartphones, journalists and media professionals rebutted. Rick Edmonds of the Poynter Institute for Media Studies called his predictions “irrationally exuberant.” John Reinan, a reporter at MinnPost, said that were Andreessen paid to analyze the news business instead of the tech industry, “His bank account would be a lot smaller.”&lt;/p&gt;
&lt;p&gt;Andreessen’s bold ideas are polarizing, but some people view him harshly on a more personal level. Bill Gurley, a partner at venture capital firm Benchmark – a firm that a16z aims to be the antithesis of – told Ben Horowitz, cofounder of a16z, to cut Andreessen out of his firm. Horowitz had also been asked by a Benchmark partner when a16z was going to get a “real C.E.O”. “If you’ve seen ‘Seinfeld,’ Bill Gurley is my Newman,” says Andreessen. Carl Icahn, a billionaire investor, is another enemy of Andreessen’s. When Andreessen was on the board of directors at Ebay, Icahn, an activist investor in the company – an investor who uses their stake in the company to influence decisions – accused Andreessen of having a conflict of interest, and wanted him removed from the board. Icahn wanted Ebay to spin off Paypal, which was previously owned by Ebay, and felt that Andreessen’s investments in Ebay’s competitors made him unsuitable to advise the board and likely to prioritize his own interests. The animosity between the two was public and somewhat petulant. Icahn mocked Andreessen’s voice, calling it so squeaky “that only a dog could understand [it],” and Andreessen compared Icahn to an “evil Captain Kirk”. In the end, Icahn won and Andreessen resigned, but the bald businessman left the incident in the past and did what he always does: predict the future.&lt;/p&gt;
&lt;p&gt;In an essay for the Wall Street Journal called “Why Software is Eating the World”, Andreessen states:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“More and more major businesses and industries are being run on software and delivered as online services – from movies to agriculture to national defense. Many of the winners are Silicon Valley-style entrepreneurial technology companies that are invading and overturning established industry structures. Over the next 10 years, I expect many more industries to be disrupted by software, with new world-beating Silicon Valley companies doing the disruption in more cases than not.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A16z’s slogan is also “Software is eating the world”, as the partners seek out the next hungry startup ready to make an impact. Today, books are software based, photography is software based, and various forms of entertainment have been eaten by software. The film on VHS became an option on Netflix; the vinyl disc became an MP3; and the game cartridge became a download on the App Store. All these changes have made products, services and experiences more accessible to the masses, but they come at a cost.&lt;/p&gt;
&lt;p&gt;Job automation is a fear that many, especially those in the working class, have. The thought of robots replacing humans en masse and wiping out large sectors of the workforce is unsettling, but Andreessen believes that there is nothing to be afraid of. Quoting the famous economist Milton Freidman, he remarks that “human wants and needs are infinite; there is always more to do”. He argues that the world is full of jobs today that never existed decades ago, and that this pattern will continue. “As consumers, we virtually never resist technological change that provides us with better products/services, even when it costs jobs,” he writes, later claiming that halting technological change stalls quality of life improvements. People will find new jobs and automation will make products and services cheaper, thus improving the lives of everyone:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Posit a world in which all material needs are provided for free, by robots and material synthesizers… Housing, energy, health care, food, transportation, [all] delivered to everyone for $0, by machines. Zero jobs in those fields remaining… Everyone enjoys a standard of living that kings and Popes could have only dreamed… all human time, labor, energy, ambition, and goals reorient to the intangibles: the big questions, the deep needs. Human nature expresses itself fully, for the first time in history. Without physical need constraints, we will be whoever we want to be.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The only way we can reach such a world is through disruption theory, which is a concept Andreessen adopted from Clay Christensen, a decorated professor of business administration at Harvard University. Disruption theory is what frightens factory workers, because it disrupts an industry with technological innovation, reducing jobs. “Had the car not disrupted the horse, we would be spending a lot more time on horses,” says Andreessen, in an a16z podcast. Christensen says that “a disruptive innovation gives new consumers access to products historically only available to consumers with a lot of money.” In Andreessen’s ideal world, human necessities are taken care of, hierarchies are flattened, and economic inequalities are reduced. Disruption theory is meant to achieve the same goals as Fordism – a system where factory workers are paid enough to afford what they produce to encourage consumption – but by exactly opposite methods. Instead of raising the wages of workers so they can afford to consume products, disruption theory advocates relieving workers of their jobs to make products cheaper so all people can access them. Andreessen claims that disruption theory places the means of production in the hands of the masses, which makes his vision sound starkly socialist, but he clarifies that he is “talking about democratic capitalism to the nth degree. [I’m not] postulating the end of money or competition or status seeking or will to power, rather the full extrapolation of each of those.”&lt;/p&gt;
&lt;p&gt;Andreessen believes three sectors are slow to embrace innovation. Healthcare is one of them, but he sees opportunities for improvement, like the application of Big Data to genomics research. The next sector is education, which has seen little change in decades, but online classes, MOOCs and even virtual reality could shake things up. The third sector is one that Andreessen can see changing more in the next three years than it has in the last twenty. This sector is financial services, and the potential overhaul is led by a recent phenomenon: Bitcoin.&lt;/p&gt;
&lt;p&gt;Bitcoin is a payment system that allows for peer-to-peer transfers of the currency without needing an intermediary, like a bank. What really makes bitcoin unique is the blockchain, a distributed database that holds, encrypts and completes transactions of bitcoin. While the blockchain is full of security benefits, its application to a currency can be problematic. Risks include the loss of a bitcoin “wallet” if a hard drive crashes or if a virus corrupts the data and the inability to reverse transactions. In addition, constant market fluctuation and a cap on the number of bitcoins - which could cause deflation - show that the cryptocurrency is far from flawless. For all the potential Andreessen sees in the product, he acknowledges that the market may not be ready. “Bitcoin is a classic venture capital endeavor: It will either work or it won’t. And if it doesn’t work, we will lose all our money. But if it does work, it will work in a spectacular way.”&lt;/p&gt;
&lt;p&gt;Some may feel that Andreessen is too cornucopian, too optimistic and too enamoured with technology to realize the supposed flaws in his thinking, but he relies on historical patterns to justify his views. He gives his line of reasoning in an interview with WIRED magazine:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“When the vast majority of the workforce was in agriculture, it was impossible to imagine what all those people would do if they didn’t have agricultural jobs. Then a hundred years later the vast majority of the workforce was in industrial jobs, and we were similarly blind: It was impossible to imagine what workers would do without those jobs. Now the majority are in information jobs. If the computers get smart enough, then what? I’ll tell you: The &lt;em&gt;then what&lt;/em&gt; is whatever we invent next.”&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Mon, 19 Oct 2015 00:00:00 -0400</pubDate>
        <link>rucs.ca/profiles-and-interviews/marc-andreessen</link>
        <guid isPermaLink="true">rucs.ca/profiles-and-interviews/marc-andreessen</guid>
        
        
        <category>Profiles and Interviews</category>
        
      </item>
    
      <item>
        <title>Lifting a Model Transformation Language</title>
        <description>&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;
&lt;script type=&quot;text/javascript&quot;
  src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;
&lt;/script&gt;

&lt;h1 class=&quot;introduction&quot;&gt;I. Introduction&lt;/h1&gt;

&lt;h3&gt;Model Driven Engineering (MDE)&lt;/h3&gt;
&lt;p&gt;MDE, the use of models to represent software artefacts, helps manage complexity in software engineering by raising the level of abstraction [1]. While MDE comprises many different languages, in simple terms, a model is a graph, a set of nodes and the associations between them. &lt;em&gt;Model transformations&lt;/em&gt; receive one model as input and produce another as output, making the changes specified by a set of &lt;em&gt;transformation rules&lt;/em&gt;. If the model fragment specified in the &lt;em&gt;match&lt;/em&gt; part of a transformation rule is found in the input, the model fragment in the &lt;em&gt;apply&lt;/em&gt; part of the rule is produced in the output. For instance, if an automotive company wanted to refactor its software, it would execute model transformations in which existing software models were the input and the transformations rules were based on refactoring techniques.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Domain Specific Language for Transformations&lt;/em&gt; (DSLTrans) is a visual language in which such transformations can be performed [4].&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/assets/KE1.png&quot; alt=&quot;&quot; /&gt;
    &lt;figcaption&gt;&lt;b&gt;Figure 1:&lt;/b&gt; Features _F0, _F1 and _F2 signify some three distinct properties of possible products. The feature model specifies that all products must have the property represented by _F0 and that the properties represented by _F1 and _F2 are mutually exclusive.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3&gt;Software Product Lines (SPLs)&lt;/h3&gt;
&lt;p&gt;SPLs help manage complexity in software engineering by concisely representing a set of similar &lt;em&gt;product variants&lt;/em&gt;, explicitly capturing product commonalities and differences [2]. For instance, the software in a car’s dashboard may differ across countries that use metric and imperial units; this necessitates the existence of a set of product variants. When product variants number in the hundreds or thousands, SPLs are particularly useful representations. Each product variant contained in an SPL corresponds to a unique combination of &lt;em&gt;features&lt;/em&gt;, which signify key functionalities of products that may differ across variants. In the case of automotive dashboard software, one feature represents metric units, another, imperial units. A &lt;em&gt;feature model&lt;/em&gt;, as seen in Figure 1, specifies constraints on the features. Some elements in an SPL might exist in some product variants and not others; associated with each element is a &lt;em&gt;presence condition&lt;/em&gt;, a boolean expression, composed of features, that represents the circumstances under which the element is present.&lt;/p&gt;

&lt;h3&gt;Research Problem&lt;/h3&gt;
&lt;p&gt;Although model transformations and SPLs are individually powerful, the transformation of SPLs is traditionally inefficient and computationally expensive [3]. Each product represented by the SPL must be derived and individually transformed, after which the set of transformed product variants is arduously abstracted into a transformed SPL. This project sought to augment the DSLTrans language so that it could support the efficient transformation of entire SPLs instead of individual models.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/assets/KE2.png&quot; alt=&quot;&quot; /&gt;
    &lt;figcaption&gt;&lt;b&gt;Figure 2:&lt;/b&gt; In this lifted transformation, the input SPL has two match sites where the transformation rule applies.  Hence, two apply sites are created.  Since there are no commonalities across the apply sites, no merging is needed.  The resulting output SPL is identical to the one that would have been created using the circuitous method of deriving individual products.&lt;/figcaption&gt;
&lt;/figure&gt; 

&lt;h1 class=&quot;approach&quot;&gt;II. Approach&lt;/h1&gt;
&lt;h3&gt;Lifting &lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Lifted transformations&lt;/em&gt;, as seen in Figure 2, operate on entire SPLs instead of individual models. In a lifted transformation, the transformation rules do not change, but the way in which they are applied to the input does [3]. &lt;em&gt;Match sites&lt;/em&gt; are regions, or subgraphs, of the input SPL where a transformation rule can be applied. For each match site, a corresponding &lt;em&gt;apply site&lt;/em&gt;, based on the apply part of the same rule, is produced. Identical elements across apply sites are merged to create the final output SPL.&lt;/p&gt;
&lt;p&gt;Identifying a match site is complicated by the fact that, depending on the presence conditions, different products may only have fragments of the match site. Appropriate presence conditions must then be constructed so that only products possessing the entirety of a match site can be transformed; the transformed products are then associated with these constructed presence conditions. In Figure 2, the product with &lt;em&gt;A&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;B&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt; only exists if feature _F1 is selected, so the corresponding transformed product, &lt;em&gt;C&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt;, has _F1 as its presence condition as well. The mutual exclusion of features _F1 and _F2 precludes &lt;em&gt;A&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;A&lt;sub&gt;2&lt;/sub&gt;&lt;/em&gt; from coexisting.&lt;/p&gt;
&lt;p&gt;This project focused on the implementation of certain aspects of lifting in an already partially lifted version of DSLTrans, namely existential matching, negative application conditions, and indirect links. Where in &lt;em&gt;universal matching&lt;/em&gt;, all of the transformed products resulting from the application of a rule can coexist, &lt;em&gt;existential matching&lt;/em&gt; precludes the coexistence of the transformed products. &lt;em&gt;Negative application conditions&lt;/em&gt; (NACs) are optional additions to a transformation rule that specify when the rule should not apply. Since any elements satisfying a NAC would invalidate an otherwise valid match site, the transformation must assert the absence of such elements. &lt;em&gt;Indirect links&lt;/em&gt; can be added to the match part of a rule to specify that some path should or should not exist between a pair of nodes, adding further expressive power to transformation rules.&lt;/p&gt;

&lt;h1 class=&quot;conclusion&quot;&gt;III. Conclusion&lt;/h1&gt;
&lt;p&gt;At the conclusion of this project, the DSLTrans language engine had been lifted successfully so that it could support transformations on entire SPLs instead of just individual models. While a partial implementation of the basic aspects of lifting existed at the onset of this project, support for existential matching, negative application conditions, and indirect links were all newly added. Mechanisms for simplifying presence conditions were also introduced, reducing the size of output presence conditions by 78.6%.&lt;/p&gt;
&lt;p&gt;Lifting the language permits modelers to use the same transformations that they have already written for individual models – in the preexisting language – on SPLs. Model transformations and SPLs have applications in areas such as the automotive and telecommunications industries, and lifting has been shown to be a scalable way of performing model transformations on an industrial scale: the technique was applied to convert realistic legacy GM models – with thousands of variants – into the AUTOSAR architecture [4]. The lifted DSLTrans language thus has the potential to be used in similar contexts, where model transformations and SPLs need to be used in conjunction in an efficient manner.&lt;/p&gt;
&lt;h4&gt;Acknowledgements&lt;/h4&gt;
&lt;p class=&quot;acknowledgements&quot;&gt;I am grateful to Claudio Gomes and Levi Lúcio for their guidance in navigating the DSLTrans and lifted DSLTrans codebases respectively. This research was funded in part by the National Science and Engineering Research Council (NSERC) of Canada.&lt;/p&gt;

&lt;h4&gt;References&lt;/h4&gt;
&lt;div class=&quot;references&quot;&gt;
    &lt;p&gt;[1] S. Sendall and W. Kozaczynski. Model Transformation: The Heart and Soul of Model-Driven Software Development. &lt;em&gt;IEEE Software&lt;/em&gt;, 20(5):42–45, 2003.  &lt;/p&gt;
    &lt;p&gt;[2] P. C. Clements and L. Northrop. &lt;em&gt;Software Product Lines: Practices and Patterns&lt;/em&gt;. SEI Ser. in SE. Addison-Wesley, 2001.  &lt;/p&gt;
    &lt;p&gt;[3] R. Salay, M. Famelis, J. Rubin, A. Di Sandro, and M. Chechik. Lifting Model  Transformations to Product Lines. In &lt;em&gt;Proc. of ICSE’14&lt;/em&gt;, pages 117–128, 2014.  &lt;/p&gt;
    &lt;p&gt;[4] Famelis, M., Lúcio, L., Selim, G., Di Sandro, A., Salay, R., Chechik, M., Cordy, J.R., Dingel, J., Vangheluwe, H. and Ramesh S: Migrating Automotive Product Lines: a Case Study. In &lt;em&gt;Proc. of ICMT’15&lt;/em&gt;, 2015.&lt;/p&gt;
&lt;/div&gt;
</description>
        <pubDate>Mon, 19 Oct 2015 00:00:00 -0400</pubDate>
        <link>rucs.ca/software-engineering/lifting-a-model-transformation-language</link>
        <guid isPermaLink="true">rucs.ca/software-engineering/lifting-a-model-transformation-language</guid>
        
        
        <category>Software Engineering</category>
        
      </item>
    
      <item>
        <title>High-Accuracy 3D Scanning by Structured Light Transport</title>
        <description>&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;
&lt;script type=&quot;text/javascript&quot;
  src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;
&lt;/script&gt;


&lt;h1 id=&quot;introduction&quot; class=&quot;unnumbered&quot;&gt;I. Introduction&lt;/h1&gt;
&lt;p&gt;An object may look different from different viewpoints. For example, a cube may look like a square from some viewpoints, but may look like a hexagon containing 3 parallelograms, each representing one face of the cube, from others. The difference between two viewpoints can be used to calculate the 3D model of the object; we call such a technique 3D reconstruction. The accuracy of the reconstructed 3D model directly depends on the accuracy of the two-view correspondence, which is the mapping between the positions of the same 3D points imaged in the two viewpoints. The most common correspondence is between two cameras, where each camera represents one of the viewpoints.&lt;/p&gt;
&lt;p&gt;However, as the path of light is reversible, we can replace one of the cameras with a projector to find the correspondence. Here, the correspondence is defined as the mapping from the pixels of the camera sensor to the pixels of the projector sensor, where the pixel in the camera is directly illuminated by the corresponding pixel in the projector. The light goes from the projector, hits the scene, and then gets reflected directly to the camera without any other reflections in the scene. Note that the correspondence is dependent on the scene.&lt;/p&gt;

&lt;figure&gt;
    &lt;figcaption &gt;&lt;b&gt;a)&lt;/b&gt;&lt;/figcaption&gt;
    &lt;img src=&quot;/assets/ZL1.jpg&quot;/&gt;
    &lt;figcaption&gt;&lt;b&gt;b)&lt;/b&gt;&lt;/figcaption&gt;
    &lt;img src=&quot;/assets/ZL2.jpg&quot;/&gt;
    &lt;figcaption&gt;&lt;b&gt;Figure 1:&lt;/b&gt; (a) shows a pair of two corresponding pixels in two cameras. The two pixels are illuminated by the same point in the scene. In (b), one of the cameras is replaced with a projector. But the two pixels are corresponding pixels because the light goes from the pixel in the projector and gets reflected only once in the scene before falling onto the pixel in the camera.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;structured-light-for-acquiring-correspondence&quot; class=&quot;unnumbered&quot;&gt;II. Structured Light for Acquiring Correspondence&lt;/h1&gt;
&lt;p&gt;The simplest way to reconstruct a 3D model is to turn on only one pixel in the projector at a time, and record the corresponding pixel in the camera, but this method is very inefficient. Our method is to assign every pixel in the projector a unique code consisting of 0’s and 1’s. As proposed by Scharstein et al. in 2003, using grey code to represent the x and y coordinates is a desirable solution. Then, we must project the code in multiple images, as shown in Figure 2. By capturing the code and decoding the pictures, the corresponding pixels on the projector can be found, and the needed correspondence can be acquired. Compared with the method of turning on only one pixel at a time, the complexity was reduced from &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{O}(n)\)&lt;/span&gt; to &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{O}(\log{n})\)&lt;/span&gt;, where n is the resolution of the projector.&lt;/p&gt;

&lt;figure&gt;
    &lt;figcaption&gt;&lt;b&gt;a)&lt;/b&gt;&lt;/figcaption&gt;
    &lt;img src=&quot;/assets/ZL3.jpg&quot;/&gt;
    &lt;figcaption&gt;&lt;b&gt;b)&lt;/b&gt;&lt;/figcaption&gt;
    &lt;img src=&quot;/assets/ZL4.jpg&quot;/&gt;
    &lt;figcaption&gt;&lt;b&gt;c)&lt;/b&gt;&lt;/figcaption&gt;
    &lt;img src=&quot;/assets/ZL5.jpg&quot;/&gt;
    &lt;figcaption&gt;&lt;b&gt;d)&lt;/b&gt;&lt;/figcaption&gt;
    &lt;img src=&quot;/assets/ZL6.jpg&quot;/&gt;
    &lt;figcaption&gt;&lt;b&gt;Figure 2:&lt;/b&gt; Assume we have a one-dimensional sensor with resolution &lt;span class=&quot;math inline&quot;&gt;\(1 \times 8\)&lt;/span&gt;. (a) shows the grey code (upper) and the binary code (lower) of the pixels. (b) through (d) are the patterns used for projector, where 1 indicates the pixel is on and 0 means off. (b) is the patterns we project when coding the pixels with grey code. Since the grey code in (a) has length of 3, we need 3 patterns. Each pattern describes only one digit of all pixels’ code. (c) is the patterns using binary code. And (d) is the patterns if we turn on only one pixel at a time.&lt;/figcaption&gt;
&lt;/figure&gt;


&lt;p&gt;Apart from using grey code, we also tried two other methods. One was replacing grey code with binary code, which produced very noisy correspondence, therefore making it very inaccurate. The other was to project 1-pixel wide horizontal or vertical lines, one line at a time. The idea is similar to turning on only one point at a time, but we used the lines to get the row index and column index. This second approach produces results as accurate as what we achieved with grey code, but it is much less efficient as its runtime is &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{O}(\sqrt{n})\)&lt;/span&gt;. In practice, using grey code only costs 5 minutes or less, but using strips needs approximately 2 hours, where the projector we use has resolution &lt;span class=&quot;math inline&quot;&gt;\(684 \times 608\)&lt;/span&gt;.&lt;/p&gt;

&lt;h1 id=&quot;indirect-invariant-imaging&quot; class=&quot;unnumbered&quot;&gt;III. Indirect-Invariant Imaging&lt;/h1&gt;
&lt;p&gt;In our previous method, we implicitly assumed that the light only travels through the direct paths, i.e. all light goes from the projector to the camera by bouncing off the scene at most once [O’Toole et al. (2014)]. However, this assumption does not hold in many cases, especially in scenes with specular surfaces, like mirrors, or scenes with transparent or semi-transparent objects, like candles. In these complex scenes, light could get reflected between objects or inside some objects. We define the light in the assumption as direct light, and the rest as indirect light.&lt;/p&gt;
&lt;p&gt;However, for any point in the projector, its corresponding point must lie on a straight line in the camera called the epipolar line, which can be calculated from the relative position of the camera and the projector, regardless of the scene. Additionally, the points in the projector corresponding to the same epipolar line lie on a straight line, which is another epipolar line. Thus, points on the epipolar line in the projector will get mapped to points on the corresponding epipolar line in the camera. This property is known as epipolar constraint.&lt;/p&gt;
&lt;p&gt;According to epipolar constraint, if only one epipolar line in the projector is turned on, and only the light on the corresponding epipolar line is captured by the camera, we can get the image with almost nothing but direct light on the epipolar line, as the randomness of indirect light makes it unlikely to fall on the epipolar line. We used a primal-dual coded camera [O’Toole et al. (2012)], which consists of a projector and a programmable mask in front of the camera, to project patterns and control which pixels can be captured in the camera. But we designed light-efficient patterns for the projector and the mask, rather than projecting only one epipolar line each time.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/assets/ZL7.jpg&quot;/&gt;
    &lt;figcaption&gt;&lt;b&gt;Figure 3:&lt;/b&gt; The sketch diagram shows the main components of our primal-dual coded camera, which are a projector, a camera, and a digital micromirror device (DMD) used as mask. The solid lines show the direct light path from the pixel in the projector to the camera, while the dashed lines show two possible indirect light paths. After calibration between the DMD mask and the projector, we can filter out most of the indirect light to get direct-light-only image.&lt;/figcaption&gt;
&lt;/figure&gt;


&lt;h1 id=&quot;future-work&quot; class=&quot;unnumbered&quot;&gt;IV. Future Work&lt;/h1&gt;

&lt;p&gt;We are going to combine the two techniques above so that we can capture an indirect-invariance image, or direct-light-only image. We can ignore the small portion of indirect light which also travels between corresponding epipolar lines when using structured light to acquire correspondence between the projector and the camera. As a result, we expect to get more accurate correspondence, especially in scenes with specular and inner reflection.&lt;/p&gt;

&lt;h4&gt;References&lt;/h4&gt;
&lt;div class=&quot;references&quot;&gt;
    &lt;p&gt;[1] M. O’Toole, J. Mather, and K. Kutulakos, ‘3D Shape and Indirect Appearance By Structured Light Transport’, &lt;em&gt;in Proc. of CVPR&lt;/em&gt;, pp. 3246-3253, 2014.&lt;/p&gt;
    &lt;p&gt;[2] M. O’Toole, R. Raskar and K. Kutulakos, ‘Primal-dual coding to probe light transport’, &lt;em&gt;TOG, vol. 31, no. 4&lt;/em&gt;, pp. 1-11, 2012.&lt;/p&gt;
    &lt;p&gt;[3] D. Scharstein and R. Szeliski, ‘High-Accuracy Stereo Depth Maps Using Structured Light’, in &lt;em&gt;Proc. of CVPR&lt;/em&gt;, pp. 1-195, 2003.&lt;/p&gt;
&lt;/div&gt;
</description>
        <pubDate>Mon, 19 Oct 2015 00:00:00 -0400</pubDate>
        <link>rucs.ca/computer-vision/high-accuracy-3D-scanning</link>
        <guid isPermaLink="true">rucs.ca/computer-vision/high-accuracy-3D-scanning</guid>
        
        
        <category>Computer Vision</category>
        
      </item>
    
      <item>
        <title>Efficient Implementation to Identify Disease Mechanisms</title>
        <description>&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;
&lt;script type=&quot;text/javascript&quot;
  src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;
&lt;/script&gt;

&lt;h1 id=&quot;introduction&quot;&gt;I. Introduction&lt;/h1&gt;
&lt;p&gt;Identifying causal genes for complex human diseases is a major challenge in the field of human genetics. As sequencing technologies become cheaper, it is becoming easier to collect genome scale data for large numbers of individuals. However, due to the natural variation across individuals, millions of genetic variants are identified through screening. Many of these variants are of little to no use for predicting the genetic causes of complex diseases, so finding causal genes using only gene mutations becomes very challenging.&lt;/p&gt;
&lt;p&gt;Other studies have looked at the gene expression levels, which are indicators of how many of each gene’s mRNA transcripts are present in the cells [1]. The biggest problem with this data is that it is very noisy and depends on a variety of environmental factors, so it is not always suitable for predicting causal genes.&lt;/p&gt;
&lt;p&gt;Recently, Mezlini et al. proposed a method that integrates gene expression and exome data to infer causal genes for complex human diseases [2]. This approach creates a hierarchical graphical model that jointly models the phenotype (disease/no disease) and gene states for all genes for each individual. A gene state is inferred from the combination of genotype (mutations) and gene expression data for each gene. This graphical model provides a complementary alternative to other approaches that use only one of the data types, i.e. exome or expression data. Combining these two results in a robust model which not only detects genes associated with the disease, but also implicates proteins that are affected in the population. However, performing inference on such a graphical model becomes a computationally intensive and time consuming task when scaled to a large number of patients.&lt;/p&gt;
&lt;p&gt;In this paper, we propose an efficient implementation of the method scalable to thousands of individuals. Ultimately, this will help in the identification of causal genes in complex human diseases.&lt;/p&gt;

&lt;h1 id=&quot;implementation&quot;&gt;II. Implementation&lt;/h1&gt;
&lt;p&gt;Implemented in R, the original probabilistic graphical model developed by Mezlini et al. is a biologically motivated hierarchical factor graph [3] that uses exome variants and mRNA expression levels as predictor variables. The graph contains many high-order factors that encode probabilistic relationships between different variables. It also contains a variety of regularization factors and factors encoding prior knowledge to build a complex and powerful model. For inference, it uses a loopy belief propagation algorithm to jointly infer the marginal distributions of unobserved variables (e.g., identity of causal genes).&lt;/p&gt;
&lt;p&gt;Time complexity for passing messages in a loopy belief propagation framework increases exponentially with the number of variables connected to the factor. Many of the high-order factors in the graphical model have several variables connected to a single variable that is only dependent on their sum. Such factors are called cardinality potentials. The original implementation uses Poisson-Binomial estimation to approximate this sum and then creates a lower order factor to pass on the messages. The time complexity for this calculation is &lt;span class=&quot;math inline&quot;&gt;\(O(n^2 \log(n))\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In our more efficient implementation, we used OpenGM, a C++ template library that provides an interface to create a bipartite factor graph that connects all factors to the corresponding variables [4]. An implementation in a compiled language like C++ removes the overhead associated with an interpreted language like R. We also implemented an improved version of the loopy belief propagation algorithm, which takes the factor graph generated by OpenGM as an input and performs inference by passing messages in a specific order, allowing the algorithm to converge faster.&lt;/p&gt;
&lt;p&gt;In addition to the implementation in C++, another optimization we made was in the evaluation of cardinality potentials. Using a divide-and-conquer approach as described by Tarlow et. al [5], we transformed the high order cardinality potential to a low order tree-structure by adding auxiliary variables. Then we framed the inference as a standard belief propagation algorithm that runs in &lt;span class=&quot;math inline&quot;&gt;\(O(n \log^2(n))\)&lt;/span&gt; time. This general class of models is called the Recursive Cardinality Model [5].&lt;/p&gt;

&lt;h1 id=&quot;analysis&quot;&gt;III. Analysis&lt;/h1&gt;
&lt;p&gt;To predict causal genes for a disease, the method needs to be trained on a dataset that contains individuals with and without the disease. Because the exome and expression data is not readily available for the same set of individuals, we used simulation data to compare the runtime of two implementations. Simulation data consisted of variants from 6900 genes for 400 individuals (200 with disease and 200 without the disease). We recorded a decrease of 40% in the overall runtime of the algorithm on average. Most of this is attributed to the optimization of the message passing in the cardinality potentials.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;IV. Conclusion&lt;/h1&gt;
&lt;p&gt;In this paper, we proposed an efficient implementation of the graphical model that combines two complementary data types to build a powerful model for the identification of causal genes. We achieved a significant reduction in the runtime, which makes the method more scalable to larger numbers of individuals. With these improvements, this method can be used for gene identification as applied to real datasets that contain the exome and expression information for patients and healthy individuals. Consequently, we expect to be able to identify novel sets of genes associated with complex human diseases. This would allow for a better understanding of underlying disease mechanisms, which could later be used to design better drugs for treatment.&lt;/p&gt;

&lt;h4&gt;References&lt;/h4&gt;
&lt;div class=&quot;references&quot;&gt;
    &lt;p&gt;[1] Anders, Simon and Huber, Wolfgang (2010). Differential expression analysis for sequence count data. , 11(10), R106.&lt;/p&gt;
    &lt;p&gt;[2] &lt;span&gt;Mezlini&lt;/span&gt;, A. M. and &lt;span&gt;Fuligni&lt;/span&gt;, F. and &lt;span&gt;Shlien&lt;/span&gt;, A. and &lt;span&gt;Goldenberg&lt;/span&gt;, A.(2015). Combining exome and gene expression datasets in one graphical model of disease to empower the discovery of disease mechanisms.&lt;/p&gt;
    &lt;p&gt;[3] Frey, Brendan J and Kschischang, Frank R and Loeliger, Hans-Andrea and Wiberg, Niclas (1997, September). Factor graphs and algorithms.. In &lt;span&gt;&lt;em&gt;Proceedings of the Annual Allerton Conference on Communication Control and Computing&lt;/em&gt;&lt;/span&gt; Vol. 35, pp. 666-680). UNIVERSITY OF ILLINOIS.&lt;/p&gt;
    &lt;p&gt;[4] Andres, B. and Beier T. and Kappes, J. H.(2012). : A &lt;span&gt;C++&lt;/span&gt; Library for Discrete Graphical Models.&lt;/p&gt;
    &lt;p&gt;[5] Tarlow, Daniel and Swersky, Kevin and Zemel, Richard S and Adams, Ryan P and Frey, Brendan J (2012). Fast Exact Inference for Recursive Cardinality Models .&lt;/p&gt;
&lt;/div&gt;</description>
        <pubDate>Mon, 19 Oct 2015 00:00:00 -0400</pubDate>
        <link>rucs.ca/computational-biology/efficient-implementation-to-identify-disease-mechanism</link>
        <guid isPermaLink="true">rucs.ca/computational-biology/efficient-implementation-to-identify-disease-mechanism</guid>
        
        
        <category>Computational Biology</category>
        
      </item>
    
  </channel>
</rss>
